Nom du fichier : Lin_2004_Rouge
Titre de l'article : ROUGE: A Package for Automatic Evaluation of Summaries 

Abstract 

ROUGE  stands  for  Recall-Oriented  Understudy for 
Gisting  Evaluation.  It  includes  measures  to  auto-
matically  determine  the  quality  of  a  summary  by 
comparing it to other (ideal) summaries created by 
humans.  The  measures  count  the  number  of  over-
lapping  units such as n-gram, word sequences, and 
word  pairs between  the  computer-generated  sum-
mary to be evaluated and the ideal summaries cre-
ated by humans. This paper introduces four different 
ROUGE measures:  ROUGE-N, ROUGE-L, ROUGE-W, 
and  ROUGE-S  included  in  the  ROUGE  summariza-
tion evaluation package and their evaluations. Three 
of  them  have  been  used  in  the  Document  Under-
standing  Conference  (DUC)  2004,  a  large-scale 
summarization evaluation sponsored by NIST. 

1 


References 

So  far,  we  only  demonstrated  how  to  compute 
ROUGE-N  using  a  single  reference.  When  multiple 
references are used, we compute pairwise summary-
level ROUGE-N between a candidate summary s and 
every reference,  ri,  in  the  reference  set.  We  then 
take  the  maximum  of  pairwise  summary-level 
ROUGE-N  scores  as  the  final  multiple  reference 
ROUGE-N score. This can be written as follows: 

 
ROUGE-Nmulti  = argmaxi ROUGE-N(ri,s)  
 
This procedure is also applied to computation of 
ROUGE-L (Section 3),  ROUGE-W  (Section  4), and 
ROUGE-S (Section 5). In the implementation, we use 
a  Jackknifing  procedure.  Given  M  references,  we 
compute  the  best  score  over  M  sets  of  M-1  refer-
ences. The final  ROUGE-N score is the average of 
the  M  ROUGE-N  scores  using  different  M-1  refer-
ences.  The Jackknifing procedure is adopted since 
we  often  need  to  compare  system  and  human  per-
formance  and  the  reference  summaries are usually 
the  only  human  summaries  available.  Using  this 
procedure,  we  are  able  to  estimate  average  human 
performance  by  averaging  M  ROUGE-N  scores  of 
one reference vs. the rest M-1 references. Although 
the Jackknifing procedure is not necessary when we 
just want to compute  ROUGE scores using multiple 
references, it is applied in all  ROUGE score compu-
tations in the ROUGE evaluation package. 

In the next section, we describe a ROUGE measure 
based  on  longest  common  subsequences  between 
two summaries. 

3  ROUGE-L: Longest Common Subsequence  

A sequence  Z  =  [z1,  z2, ..., zn] is a subsequence of 
another sequence X = [x1, x2, ..., xm], if there exists a 
strict increasing sequence [i1, i2, ..., ik] of indices of 
X  such  that  for  all  j =  1,  2,  ...,  k, we have  xij  =  zj  
(Cormen et al., 1989). Given two sequences X and 
Y, the longest common subsequence (LCS) of X and 

Y is a common subsequence with maximum length. 
LCS  has  been  used  in  identifying  cognate  candi-
dates during construction  of N-best translation lexi-
con  from  parallel  text.  Melamed  (1995)  used  the 
ratio (LCSR) between the length of the LCS of two 
words and the length of the longer word of the two 
words  to  measure  the  cognateness  between  them. 
He  used  LCS  as  an  approximate  string matching 
algorithm.  Saggion  et  al.  (2002)  used  normalized 
pairwise  LCS  to  compare  similarity  between  two 
texts in automatic summarization evaluation.  

3.1  Sentence-Level LCS 

To  apply  LCS  in  summarization  evaluation,  we 
view a summary sentence as a sequence  of words. 
The  intuition  is  that  the  longer  the  LCS  of  two 
summary  sentences  is,  the  more  similar  the  two 
summaries  are.  We  propose  using  LCS-based  F-
measure  to  estimate  the  similarity  between  two 
summaries X of length  m and  Y of length n, assum-
ing  X  is  a  reference summary sentence and  Y is a 
candidate summary sentence, as follows: 

 

      (2) 

      (3) 

LCS

LCS

+
1(
R
lcs

,

,

)

)

YX
(
m
YX
(
n
2)
b
PR
lcs
lcs
+
2
b
P
lcs

(4) 

=

=

Rlcs 

Plcs 

=

Flcs  

 

Where  LCS(X,Y)  is  the  length  of  a  longest  com-
mon subsequence of X and Y, and ß = Plcs/Rlcs when 
?Flcs/?Rlcs_=_?Flcs/?Plcs.  In DUC,  ß is set to  a very 
big  number  (?  8 ) . Therefore, only  Rlcs  is  consid-
ered. We call the LCS-based F-measure, i.e. Equa-
tion 4, ROUGE-L. Notice that ROUGE-L is 1 when X 
= Y; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. 
there  is  nothing  in  common  between  X and  Y.  F-
measure or its equivalents has been shown to have 
met  several  theoretical  criteria  in  measuring  accu-
racy  involving  more  than  one  factor  (Van  Rijsber-
gen,  1979).  The  composite  factors  are  LCS-based 
recall  and  precision  in  this  case.  Melamed  et  al. 
(2003) used unigram F-measure to estimate machine 
translation  quality  and  showed  that  unigram  F-
measure was as good as BLEU.  

One advantage of using LCS is that it does not re-
quire consecutive matches but in-sequence matches 
that  reflect  sentence  level  word  order  as  n-grams. 
The other advantage is that it automatically includes 
longest  in-sequence common n-grams, therefore no 
predefined n-gram length is necessary.  

ROUGE-L as defined in Equation 4 has the prop-
erty that its value is less than or equal to the mini-
mum  of  unigram  F-measure of  X and  Y. Unigram 

recall  reflects  the  proportion  of  words  in  X  (refer-
ence summary sentence) that are also present in  Y 
(candidate  summary  sentence);  while  unigram  pre-
cision is the proportion of words in Y that are also in 
X.  Unigram  recall  and  precision  count  all  co-
occurring  words  regardless  their  orders;  while 
ROUGE-L counts only in-sequence co-occurrences.  
By  only  awarding  credit  to  in-sequence  unigram 
matches,  ROUGE-L  also  captures  sentence  level 
structure  in  a  natural  way.  Consider  the  following 
example: 

 

S1.  police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
 

We only consider ROUGE-2, i.e. N=2, for the pur-
pose of explanation. Using S1 as the reference and 
S2 and S3 as the candidate  summary sentences, S2 
and S3 would have the same  ROUGE-2 score, since 
they both have one bigram, i.e. “the gunman”. How-
ever, S2 and S3 have very different meanings. In the 
case of ROUGE-L, S2 has a score of 3/4 = 0.75 and 
S3 has a score of 2/4 = 0.5, with ß = 1. Therefore S2 
is better than S3 according to ROUGE-L. This exam-
ple also illustrated that  ROUGE-L can work reliably 
at sentence level. 

However,  LCS  suffers  one  disadvantage  that  it 
only counts the main in-sequence words; therefore, 
other  alternative  LCSes  and  shorter  sequences are 
not reflected in the final score. For example, given 
the following candidate sentence: 
S4. the gunman police killed 

Using S1 as its reference, LCS counts either “the 
gunman” or “police killed”, but not both; therefore, 
S4  has  the same  ROUGE-L score as S3.  ROUGE-2 
would prefer S4 than S3. 

3.2  Summary-Level LCS 

Previous  section  described  how  to  compute  sen-
tence-level  LCS-based  F-measure  score.  When  ap-
plying  to  summary-level,  we  take  the  union  LCS 
matches between a reference summary sentence, ri, 
and every  candidate summary sentence,  cj. Given a 
reference summary of u sentences containing a total 
of m words and a candidate summary of v sentences 
containing  a  total  of  n  words,  the  summary-level 
LCS-based F-measure can be computed as follows: 

u

Rlcs 

=

=
1

i

u

Plcs 

=

=
1

i

LCS

Cr
,(
i

)

      (5) 

m

LCS

Cr
,(
i

)

      (6) 

n

+
1(
R
lcs

2)
b
PR
lcs
lcs
+
2
b
P
lcs

   (7) 

=

Flcs  

 

i

)

LCS

,( Cr

Again  ß  is  set  to  a  very  big  number  (?  8 )  in 
is the 
DUC, i.e. only  Rlcs is considered. 
LCS  score  of  the  union  longest  common  subse-
quence between  reference sentence  ri and candidate 
summary C. For example, if ri = w1 w2 w3 w4 w5, and 
C contains two sentences: c1 = w1 w2 w6 w7 w8 and c2 
= w1 w3 w8 w9 w5, then  the longest common subse-
quence of ri and  c1 is “w1 w2” and the longest com-
mon  subsequence of  ri and  c2  is  “w1  w3  w5”. The 
union  longest common subsequence of ri, c1, and  c2 
is “w1 w2 w3 w5” and 

= 4/5. 

,( Cr

LCS

)

i

3.3  ROUGE-L vs. Normalized Pairwise LCS 

The normalized pairwise LCS proposed by Radev et 
al. (page 51, 2002) between two summaries S1 and 
S2, LCS(S1 ,S2)MEAD , is written as follows: 

 

max

s

j

S

2

LCS

(
s

,

s

)

j

i

s

i

S

1

max

s

i

S

1

LCS

(
s

,

s

i

)

j

 (8) 

s

j

S

2

length

(

s

)

i

s

i

S

1

length

(
s

)

j

s

j

S

2

 
Assuming  S1 has  m  words  and  S2  has  n words, 
Equation  8  can  be  rewritten  as  Equation  9  due  to 
symmetry: 

+

+

 

 

*2

Ss
i
1

max

LCS

(

s

,

s

j

i

)

2

                       (9) 

j

S
nm

s
+

 
We then define MEAD LCS recall (Rlcs-MEAD) and 

MEAD LCS precision (Plcs-MEAD) as follows: 

 Rlcs-MEAD 

=

max

Ss
i
1

s

j

2

S
m

LCS

(

ss
,
i

j

)

      (10) 

=

2

S
n

max

s

j

Ss
i
1

LCS

(

ss
,
i

j

)

Plcs-MEAD 
 
We can rewrite Equation (9) in terms of  Rlcs-MEAD 
and Plcs-MEAD with a constant parameter ß = 1 as fol-
lows: 

       (11) 

LCS(S1 ,S2)MEAD  

=

+
1(
R
lcs

b

2 )

R
lcs
+

P
MEAD
lcs
b
P
lcs

2

MEAD

MEAD

 (12) 

MEAD

Equation 12 shows that normalized pairwise LCS 
as defined in Radev et al. (2002) and implemented 
in MEAD is also a F-measure with ß = 1. Sentence-
level  normalized  pairwise  LCS  is  the  same  as 
ROUGE-L with  ß  =  1.  Besides  setting  ß  =  1,  sum-
mary-level  normalized  pairwise  LCS  is  different 
from ROUGE-L in how a sentence gets its LCS score 
from its references. Normalized pairwise LCS takes 

(cid:229)
¨
(cid:229)
¨
¨
¨
(cid:229)
(cid:229)
(cid:229)
(cid:229)
˛
˛
˛
˛
˛
˛
(cid:229)
˛
˛
(cid:229)
˛
˛
(cid:229)
˛
˛
-
-
-
-
the best LCS score while  ROUGE-L takes the union 
LCS score. 

4  ROUGE-W: Weighted Longest Common Sub-

sequence 

LCS has many nice properties as we have described 
in  the  previous  sections.  Unfortunately,  the  basic 
LCS also has a problem that it does not differentiate 
LCSes of different spatial relations within their em-
bedding sequences. For example, given a reference 
sequence X and two candidate sequences  Y1 and  Y2 
as follows: 

 
[A B C D E F G] 
X:  
Y1: 
[A B C D H I K] 
Y2:   [A H B K C I D] 
 
Y1 and  Y2  have the same  ROUGE-L  score.  How-
ever, in this case, Y1 should be the better choice than 
Y2 because Y1 has consecutive matches. To improve 
the basic LCS method, we can simply remember the 
length of consecutive matches encountered so far to 
a  regular  two  dimensional  dynamic  program  table 
computing  LCS.  We  call  this  weighted  LCS 
(WLCS) and use k to indicate the length of the cur-
rent consecutive matches ending at words  xi and  yj. 
Given two sentences X and Y, the WLCS score of X 
and Y can be computed using the following dynamic 
programming procedure: 

 
(1) For (i = 0; i <=m; i++) 
        c(i,j) = 0  // initialize c-table 
        w(i,j) = 0 // initialize w-table 
(2) For (i = 1; i <= m; i++) 
        For (j = 1; j <= n; j++) 
          If xi = yj Then 

     // the length of consecutive matches at 
     // position i-1 and j -1 
     k = w(i-1,j-1) 
     c(i,j) = c(i-1,j-1) + f(k+1) – f(k) 
     // remember the length of consecutive 
     // matches at position i, j  
     w(i,j) = k+1 

          Otherwise 

     If c(i-1,j) > c(i,j-1) Then 

    c(i,j) = c(i-1,j) 
    w(i,j) = 0           // no match at i, j 

     Else c(i,j) = c(i,j-1) 
     w(i,j) = 0           // no match at i, j 

(3) WLCS(X,Y) = c(m,n) 
 

Where  c  is  the  dynamic  programming  table,  c(i,j) 
stores the WLCS score ending at word xi of X and yj 
of Y, w is the table storing the length of consecutive 
matches ended at c table position  i and  j, and  f is a 
function  of  consecutive  matches  at  the  table  posi-

tion,  c(i,j).  Notice  that  by  providing  different 
weighting  function  f,  we  can  parameterize  the 
WLCS  algorithm  to  assign  different  credit  to  con-
secutive in-sequence matches.  

The weighting function  f must have the property 
that  f(x+y) >  f(x) +  f(y)  for  any  positive  integers  x 
and  y.  In  other  words,  consecutive  matches  are 
awarded more scores than non-consecutive matches. 
For example, f(k)-=-ak – b when k >= 0, and a, b > 
0.  This  function  charges  a  gap  penalty of  –b for 
each  non-consecutive  n-gram  sequences.  Another 
possible function family is the polynomial family of 
the form  k a  where  -a  >  1.  However,  in  order  to 
normalize the final  ROUGE-W score, we also prefer 
to  have  a  function  that  has  a  close  form  inverse 
function.  For  example,  f(k)-=-k 2  has  a  close  form 
inverse function  f  -1(k)-=-k 1/2.   F-measure  based  on 
WLCS  can  be  computed  as  follows,  given  two  se-
quences X of length m and Y of length n: 

Rwlcs 

= -
f

Pwlcs 

= -
f

1

1

+
1(
R

b

wlcs

Fwlcs  

=

 

,

,

YX
)
YX

(
WLCS
(
mf
WLCS
(
)(
nf
PR
wlcs
2
b
P
wlcs

2)
+

wlcs

)

)

      (13) 

      (14) 

          (15) 

Where f  -1 is the inverse function of  f. In DUC,  ß is 
set  to  a  very  big  number  (?  8 ) .  Therefore, only 
Rwlcs  is  considered.  We  call  the  WLCS-based  F-
measure, i.e. Equation 15,  ROUGE-W. Using Equa-
tion 15 and  f(k)-=-k2 as the weighting function, the 
ROUGE-W scores for sequences Y1 and  Y2 are 0.571 
and  0.286  respectively.  Therefore,  Y1  would  be 
ranked  higher  than  Y2  using  WLCS.  We  use  the 
polynomial  function of the form  k a  in the  ROUGE 
evaluation package.  In  the  next  section,  we  intro-
duce the skip-bigram co-occurrence statistics. 

5  ROUGE-S:  Skip-Bigram  Co-Occurrence  Sta-

tistics 

Skip-bigram  is  any  pair  of  words  in  their  sentence 
order,  allowing  for  arbitrary  gaps.  Skip-bigram  co-
occurrence  statistics  measure  the  overlap  of  skip-
bigrams between a candidate translation and a set of 
reference translations. Using the  example  given  in 
Section 3.1: 

 

S1.  police killed the gunman 
S2. police kill the gunman 
S3. the gunman kill police 
S4. the gunman police killed 

(cid:247)
(cid:247)
ł
(cid:246)
(cid:231)
(cid:231)
Ł
(cid:230)
(cid:247)
(cid:247)
ł
(cid:246)
(cid:231)
(cid:231)
Ł
(cid:230)
each sentence has C(4,2)1 = 6 skip-bigrams. For ex-
ample, S1 has the following skip-bigrams: 

(“police  killed”,  “police  the”,  “police  gunman”, 

“killed the”, “killed gunman”, “the gunman”)  

S2  has  three  skip-bigram  matches  with  S1  (“po-
lice the”, “police gunman”, “the gunman”), S3 has 
one skip-bigram match with S1 (“the gunman”), and 
S4  has  two  skip-bigram  matches  with  S1  (“police 
killed”,  “the  gunman”).    Given  translations  X  of 
length  m and Y  of  length n, assuming  X is a refer-
ence translation and  Y is a candidate translation, we 
compute skip-bigram-based F-measure as follows: 

)

)

,
YX
SKIP
(2
)2,
(
mC
SKIP
YX
(2
,
)2,(
nC
+
2)
b
P
1(
R
skip
2
skip
+
2
b
R
P
skip

skip

2

2

          (16) 

          (17) 

2

  (18) 

Rskip2 

Pskip2 

Fskip2 

=

=

=

 

Where SKIP2(X,Y) is the number of skip-bigram 
matches between  X and Y, ß controlling the relative 
importance of Pskip2 and Rskip2, and  C is the combi-
nation  function.  We  call  the  skip-bigram-based  F-
measure, i.e. Equation 18, ROUGE-S. 

Using Equation 18 with ß = 1 and S1 as the refer-
ence, S2’s  ROUGE-S  score  is  0.5,  S3  is  0.167, and 
S4 is 0.333. Therefore, S2 is better than S3 and S4, 
and S4 is better than S3. This result is more intuitive 
than using BLEU-2 and ROUGE-L. One advantage of 
skip-bigram vs. BLEU is that it does not require con-
secutive matches but is still sensitive to word order. 
Comparing  skip-bigram  with  LCS,  skip-bigram 
counts all in-order matching word pairs while LCS 
only counts one longest common subsequence. 

Applying  skip-bigram  without  any  constraint  on 
the  distance  between  the  words,  spurious  matches 
such as  “the  the”  or  “of  in”  might  be  counted  as 
valid  matches.  To  reduce  these  spurious matches, 
we can limit the maximum skip distance,  dskip,  be-
tween two in-order words that is allowed to form a 
skip-bigram.  For  example,  if  we  set  dskip to 0 then 
ROUGE-S  is  equivalent 
to  bigram  overlap  F-
measure. If we set dskip to 4 then only word pairs of 
at most 4 words apart can form skip-bigrams. 

Adjusting Equations 16, 17, and 18 to use maxi-
mum skip distance limit is straightforward: we only 
count the skip-bigram matches,  SKIP2(X,Y), within 
the  maximum  skip  distance  and  replace  denomina-
tors of Equations 16,  C(m,2), and 17,  C(n,2), with 
the actual numbers of within distance skip-bigrams 
from the reference and the candidate respectively.  

 

 

5.1  ROUGE-SU: Extension of ROUGE-S 

One  potential  problem for  ROUGE-S is that it  does 
not  give  any  credit  to  a  candidate  sentence  if  the 
sentence does not have any word pair co-occurring 
with its references. For example, the following sen-
tence has a ROUGE-S score of zero: 

S5. gunman the killed police 

 
S5  is  the  exact  reverse  of  S1  and  there  is  no  skip 
bigram  match  between  them. However,  we  would 
like  to  differentiate  sentences  similar  to  S5  from 
sentences  that  do  not  have  single  word  co-
occurrence  with  S1.  To  achieve  this,  we  extend 
ROUGE-S with the addition of unigram as counting 
unit. The extended version is called ROUGE-SU. We 
can also obtain ROUGE-SU from ROUGE-S by add-
ing a begin-of-sentence marker at the beginning of 
candidate and reference sentences. 

6  Evaluations of ROUGE 

To assess the effectiveness of ROUGE measures,  we 
compute  the  correlation  between  ROUGE  assigned 
summary  scores  and  human  assigned  summary 
scores. The intuition is that a good evaluation meas-
ure should assign a good score to a  good summary 
and a bad score to a bad summary. The ground truth 
is based on  human  assigned  scores.  Acquiring  hu-
man judgments  are  usually  very  expensive;  fortu-
nately,  we  have  DUC  2001,  2002,  and  2003 
evaluation  data  that  include  human  judgments for 
the following: 
•  Single  document  summaries  of  about  100 
words:  12 systems 2 for DUC 2001  and 14 sys-
tems for 2002. 149 single document summaries 
were judged per system in DUC 2001 and  295 
were judged in DUC 2002. 

•  Single document very short summaries of about 
10 words (headline-like, keywords, or phrases): 
14 systems for DUC 2003. 624 very short sum-
maries were judged per system in DUC 2003. 

•  Multi-document summaries of  about  10 words: 
6 systems for DUC 2002; 50 words: 14 systems 
for  DUC  2001  and  10  systems  for  DUC  2002; 
100 words: 14 systems for DUC 2001,  10 sys-
tems  for  DUC 2002, and 18 systems for DUC 
2003; 200 words: 14 systems for DUC 2001 and 
10 systems for  DUC 2002;  400 words: 14 sys-
tems for DUC 2001. 29 summaries were judged 
per system  per summary size  in DUC 2001, 59 
were judged in DUC 2002, and 30 were judged 
in DUC 2003. 

                                                                 

1 C(4,2) = 4!/(2!*2!) = 6. 

                                                                 

2 All systems include 1 or 2 baselines. Please see DUC 

website for details. 

DUC 2001 100 WORDS SINGLE DOC

DUC 2002 100 WORDS SINGLE DOC

1 REF

3  REFS

1 REF

2 REFS

Method CASE STEM STOP CASE STEM STOP CASE STEM STOP CASE STEM STOP
0.76
R-1
0.99
0.84
0.99
R-2
0.82
0.99
R-3
0.81
0.99
R-4
0.79
0.98
R-5
0.76
0.98
R-6
0.73
0.97
R-7
0.69
0.97
R-8
0.65
R-9
0.96
0.83
0.99
R-L
0.74
0.98
R-S*
0.84
0.99
R-S4
0.84
0.99
R-S9
0.74
0.98
R-SU*
0.99
0.84
R-SU4
0.99
R-SU9
0.84
R-W-1.2 0.85
0.99

0.78
0.87
0.86
0.84
0.83
0.81
0.80
0.78
0.76
0.86
0.77
0.88
0.88
0.77
0.87
0.87
0.87

0.80
0.87
0.86
0.84
0.83
0.81
0.79
0.78
0.76
0.86
0.78
0.87
0.87
0.78
0.87
0.87
0.87

0.98
0.99
0.99
0.99
0.99
0.98
0.98
0.98
0.97
0.99
0.98
0.99
0.99
0.98
0.99
0.99
0.99

0.98
0.99
0.99
0.99
0.99
0.99
0.98
0.98
0.97
0.99
0.98
0.99
0.99
0.98
0.99
0.99
0.99

0.76
0.84
0.83
0.81
0.79
0.77
0.74
0.71
0.67
0.83
0.74
0.85
0.85
0.74
0.84
0.84
0.85

0.84
0.83
0.80
0.77
0.75
0.71
0.65
0.61
0.59
0.83
0.80
0.84
0.84
0.81
0.85
0.85
0.85

0.99
0.99
0.99
0.98
0.98
0.97
0.97
0.96
0.95
0.99
0.98
0.99
0.99
0.98
0.99
0.99
0.99

0.98
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.98
0.99
0.98
0.99
0.99
0.98
0.99
0.99
0.99

0.98
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.98
0.99
0.97
0.99
0.99
0.98
0.99
0.99
0.99

0.84
0.86
0.85
0.83
0.81
0.79
0.76
0.72
0.69
0.86
0.82
0.87
0.87
0.83
0.87
0.87
0.87

Table  1:  Pearson’s  correlations  of  17  ROUGE
measure scores vs. human judgments for the DUC 
2001  and  2002  100  words  single  document sum-
marization tasks 

DUC 2003 10 WORDS SINGLE DOC

1 REF 4REFS 1 REF 4 REFS 1 REF 4 REFS

CASE

STEM

STOP

Method
0.96
R-1
0.75
R-2
0.71
R-3
0.64
R-4
0.62
R-5
0.57
R-6
0.56
R-7
0.55
R-8
0.51
R-9
0.97
R-L
0.89
R-S*
0.88
R-S4
0.92
R-S9
0.93
R-SU*
0.97
R-SU4
R-SU9
0.97
R-W-1.2 0.96

0.95
0.76
0.70
0.65
0.64
0.62
0.56
0.53
0.47
0.96
0.87
0.89
0.92
0.90
0.96
0.95
0.96

0.95
0.75
0.70
0.62
0.60
0.55
0.58
0.54
0.51
0.97
0.88
0.88
0.92
0.91
0.96
0.96
0.96

0.95
0.75
0.68
0.63
0.63
0.61
0.60
0.55
0.49
0.96
0.85
0.88
0.91
0.89
0.95
0.94
0.96

0.90
0.76
0.73
0.69
0.63
0.46
0.46
0.00
0.00
0.97
0.95
0.95
0.97
0.96
0.98
0.97
0.96

0.90
0.77
0.70
0.66
0.60
0.54
0.44
0.24
0.14
0.96
0.92
0.96
0.95
0.94
0.97
0.95
0.96

Table  2:  Pearson’s  correlations  of  17  ROUGE
measure scores vs. human judgments for the DUC 
2003 very short summary task 

Besides these human judgments, we also have 3 sets 
of  manual  summaries  for  DUC  2001,  2  sets  for 
DUC  2002,  and  4  sets  for  DUC  2003.  Human 
judges assigned content coverage scores to a candi-
date summary by examining the percentage of con-
tent  overlap  between  a  manual  summary  unit,  i.e. 
elementary discourse unit or sentence, and the can-
didate  summary  using  Summary  Evaluation  Envi-
ronment3  (SEE)  developed  by    the  University  of 
Southern California’s Information Sciences Institute 
(ISI).  The  overall  candidate  summary  score  is  the 
average  of  the  content  coverage  scores  of  all  the 
units  in  the  manual  summary.  Note  that  human 
judges  used  only  one  manual  summary  in  all  the 
evaluations although multiple alternative summaries 
were available. 

With  the  DUC  data,  we  computed  Pearson’s 
product  moment  correlation  coefficients,  Spear-
man’s  rank  order  correlation  coefficients,  and 
Kendall’s correlation coefficients between systems’ 
average  ROUGE  scores  and  their  human  assigned 
average coverage scores using single reference and 
multiple  references.  To  investigate  the  effect  of 
stemming and inclusion or exclusion of stopwords, 
we also ran experiments over original automatic and 

                                                                 

3 SEE is available online at http://www.isi.edu/~cyl. 

manual summaries (CASE set), stemmed4 version of 
the summaries (STEM set), and stopped version of 
the summaries (STOP set).  For  example,  we  com-
puted ROUGE scores for the 12 systems participated 
in  the  DUC  2001  single  document  summarization 
evaluation using the CASE set with single reference 
and then  calculated  the  three correlation scores for 
these 12 systems’ ROUGE scores vs. human assigned 
average coverage scores. After that we repeated the 
process  using  multiple  references  and  then  using 
STEM and STOP sets. Therefore, 2 (multi or single) 
x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spear-
man, or Kendall) =  18 data points were collected for 
each ROUGE measure and each DUC task. To assess 
the significance of the results,  we applied bootstrap 
resampling technique (Davison and Hinkley, 1997) 
to estimate 95% confidence intervals for every cor-
relation computation. 

17 ROUGE measures were tested for each run us-
ing  ROUGE  evaluation  package  v1.2.1:  ROUGE-N  
with  N  =  1  to  9,  ROUGE-L,  ROUGE-W  with 
weighting factor a  = 1.2, ROUGE-S and ROUGE-SU 
with maximum skip distance dskip = 1, 4, and 9. Due 
to  limitation  of  space,  we  only  report  correlation 
analysis results based on Pearson’s correlation coef-
ficient.  Correlation  analyses  based  on  Spearman’s 
and  Kendall’s  correlation  coefficients  are  tracking 
Pearson’s very closely and will be posted later at the 
ROUGE  website5  for  reference.  The  critical  value6 
for Pearson’s correlation is 0.632 at 95% confidence 
with 8 degrees of freedom. 

Table  1  shows  the  Pearson’s  correlation  coeffi-
cients of the 17  ROUGE measures vs. human  judg-
ments  on  DUC  2001  and  2002  100  words single 
document  summarization  data.  The  best  values  in 
each column are marked with dark (green) color and 
statistically equivalent values  to the best values are 
marked with gray. We found that  correlations were 
not affected by stemming or removal of stopwords 
in  this  data  set,  ROUGE-2  performed  better among 
the  ROUGE-N variants,  ROUGE-L,  ROUGE-W, and 
ROUGE-S were all performing well, and using mul-
tiple  references  improved  performance  though  not 
much.  All  ROUGE  measures  achieved  very  good 
correlation with human judgments in the DUC 2002 
data. This  might due to  the  double  sample  size  in 
DUC 2002 (295 vs. 149 in DUC 2001) for each sys-
tem. 

Table 2 shows the correlation analysis results on 
the DUC 2003 single document very short summary 
data. We found that  ROUGE-1,  ROUGE-L,  ROUGE-

                                                                 

4 Porter’s stemmer was used. 
5 ROUGE website: http://www.isi.edu/~cyl/ROUGE. 
6  The  critical values for Pearson’s correlation at 95% 
confidence  with 10, 12, 14, and 16 degrees of freedom 
are 0.576, 0.532, 0.497, and 0.468 respectively. 

(A1) DUC 2001 100 WORDS MULTI

(A2) DUC 2002 100 WORDS MULTI

(A3) DUC 2003 100 WORDS MULTI

1 R